{
  "hash": "12cc01d75bc38e3ba95e89ca06708259",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"时间序列分析：第二章\"\nsubtitle: \"时间序列回归和探索性数据分析\"\nauthor: \"周博霖\"\ndate: \"2024/9/11\"\ncategories: [学习笔记,统计方法,时间序列分析]\nimage: \"cover.png\"\nbibliography: references.bib\ncsl: apa.csl\nnumber-sections: true\ntoc: true\ntoc-depth: 2\n---\n\n\n\n\n主要参考书籍为Shumway和Stoffer[-@Shumway_2017]的Time Series Analysis and Its Applications: With R Examples。\n\n该书第五版正在制作，配套`astsa`包中的部分数据集会逐渐更新，所以后期也许会换成第五版内容。\n\n该内容为学习笔记，会很碎片化，同时参考了作者在[github](https://github.com/nickpoison/tsa4/blob/master/textRcode.md)上给出的代码。\n\n::: callout-tip\n提示框表明这部分完全是个人理解，可能有误。\n\n一切错误和想讨论的内容欢迎联系邮箱(zhoubolin0404\\@126.com)，感谢！\n:::\n\n在本章中，我们将介绍时间序列背景下的经典多元线性回归、模型选择、非平稳时间序列预处理(如趋势去除)的探索性数据分析、差分(differencing)和后移算子(backshift operator)的概念、方差稳定和时间序列的非参数平滑。\n\n# 时间序列背景下的经典回归\n\n对于时间序列$x_t$($t=1,\\cdots,n$)，受到一组可能的输入或自变量序列($z_{t1},z_{t2},\\cdots,z_{tq}$)的影响。用线性回归模型来考虑这种关系，则模型为：\n\n$$\nx_t=\\beta_0+\\beta_1z_{t1}+\\beta_2z_{t2}+\\cdots+\\beta_qz_{tq}+w_t\n$$\n\n其中$\\beta_0,\\beta_1,\\cdots,\\beta_q$是固定的回归系数，$w_t$是随机误差或噪声过程，均值为$0$，方差为$\\sigma_w^2$，正态分布。\n\n::: callout-tip\n$z_{t}$一般就是时间。\n:::\n\n## 简单线性回归\n\n[@fig-chicken]展示了2001年8月到2016年7月美国某码头鸡肉每月价格，直线是线性回归的拟合。\n\n\n::: {.cell}\n::: {.cell-output-display}\n![鸡肉价格](Chapter_2_files/figure-html/fig-chicken-1.png){#fig-chicken width=672}\n:::\n:::\n\n\n拟合模型为：\n\n$$\nx_t=\\beta_0+\\beta_1z_t+w_t,\\ z_t=2001\\frac{7}{12},2001\\frac{8}{12},\\cdots,2016\\frac{6}{12}\n$$\n\n其中$w_t$被假设为一个独立同分布的正态分布序列(如不成立需要进行额外操作，第三章会介绍)。\n\n::: callout-tip\n没看懂为什么是$2001\\frac{7}{12}$到$2016\\frac{6}{12}$。\n\n之后是对一元线性回归的计算，再后面是多元的，方法类似，故不对一元的进行解释，直接给出结果。\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit <- lm(chicken ~ time(chicken), na.action = NULL))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = chicken ~ time(chicken), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7411 -3.4730  0.8251  2.7738 11.5804 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -7.131e+03  1.624e+02  -43.91   <2e-16 ***\ntime(chicken)  3.592e+00  8.084e-02   44.43   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.696 on 178 degrees of freedom\nMultiple R-squared:  0.9173,\tAdjusted R-squared:  0.9168 \nF-statistic:  1974 on 1 and 178 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n结果表明：斜率系数$\\hat\\beta_1=3.59$(标准误差为$0.08$)，$p<0.001$，结果显著。\n\n::: callout-tip\n接下来会论述多元线性回归的计算过程，方便引出Akaike信息准则(Akaike’s Information Criterion，AIC)，个人感觉不知道怎么推导也没什么问题。仅从个人理解角度进行概括，看不懂可以跳过，或者给出修改建议。\n:::\n\n## 多元线性回归\n\n多元线性回归模型，$z_t=(z_{t1},\\ z_{t2},\\ \\cdots,\\ z_{tq})'$和$\\beta=(\\beta_0,\\ \\beta_1,\\ \\cdots,\\ \\beta_q)'$这种列向量表示起来更为方便。\n\n模型可以表示为：\n\n$$\nx_t=\\beta_0+\\beta_1z_{t1}+\\beta_2z_{t2}+\\cdots+\\beta_qz_{tq}+w_t=\\beta'z_t+w_t\n$$\n\n其中$w_t\\sim\\operatorname{iid\\ N}(0,\\ \\sigma_w^2)$。\n\n普通最小二乘法(Ordinary Least Square，OLS)下，最小化的误差平方和(residual sum of squares或sum of squares for error，RSS或SSE)为：\n\n$$\nQ=\\sum_{t=1}^{n}w_t^2=\\sum_{t=1}^{n}(x_t-\\beta'z_t)^2\n$$\n\n最小化($Q$关于$\\beta$的偏导数为0)需要满足$\\sum_{t=1}^n(x_t-\\hat\\beta'z_t)z_t'=0$(表示$w_t$和$z_t$不相关)，得到正规方程(normal equations)：\n\n$$\n(\\sum_{t=1}^{n}z_tz_t')\\hat\\beta=\\sum_{t=1}^{n}z_tx_t\n$$\n\n对这个方程求解就可以得到：\n\n$$\n\\hat\\beta=(\\sum_{t=1}^{n}z_tz_t')^{-1}\\sum_{t=1}^{n}z_tx_t\n$$\n\n所以最小化的误差平方和为：\n\n$$\n\\operatorname{SSE}=\\sum_{t=1}^{n}(x_t-\\hat\\beta'z_t)^2\n$$\n\n这是无偏的($\\operatorname{E}(\\hat\\beta)=\\beta$)，且具有最小方差。\n\n如果误差$w_t$是正态分布，$\\hat\\beta$是$\\beta$的最大似然估计，且具有正态分布\n\n$$\n\\operatorname{cov}(\\hat\\beta)=\\sigma_w^2C\n$$\n\n其中\n\n$$\nC=(\\sum_{t=1}^{n}z_tz_t')^{-1}\n$$\n\n$C$是$z_t$的协方差矩阵的逆。\n\n方差$\\sigma_w^2$的无偏估计是\n\n$$\ns_w^2=\\operatorname{MSE}=\\frac{\\operatorname{SSE}}{n-(q+1)}\n$$\n\n$\\operatorname{SSE}$表示均方误差(mean squared error)。正态假设下\n\n$$\nt=\\frac{(\\hat\\beta_i-\\beta_i)}{s_w\\sqrt{c_{ii}}}\n$$\n\n服从自由度为$n-(q+1)$的$t$分布，$c_{ii}$表示矩阵$C$的第$i$个对角线元素。对于$i=1,\\ \\cdots,\\ q$，该结果常用于检验零假设$\\operatorname{H_0}:\\beta_t=0$.\n\n竞争模型关注隔离或选择最佳自变量子集点。选择$r<q$个独立模型，即$z_{t,1:r}={z_{t1},\\ z_{t2},\\ \\cdots,\\ z_{tr}}$。模型为\n\n$$\nx_t=\\beta_0+\\beta_1z_{t1}+\\cdots+\\beta_rz_{tr}+w_t\n$$\n\n零假设为$\\operatorname{H_0}:\\beta_{r+1}=\\cdots=\\beta_q=0$，使用$F$检验\n\n$$\nF=\\frac{(\\operatorname{SSE}_r-\\operatorname{SSE}/(q-r))}{\\operatorname{SSE}/(n-q-1)}=\\frac{\\operatorname{MSR}}{\\operatorname{MSE}}\n$$\n\n在自由度为$q-r$和$n-q-1$的$F$分布中进行检验。\n\n整体逻辑是看添加上其他自变量，是否显著增加拟合效果。如果$\\operatorname{H}_0$为真，则可以化简模型，反之则不可以。\n\n|    误差源     | 自由度(df) |                            平方和                            |                      均方                       |                       $F$值                       |\n|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n| $z_{t,r+1:q}$ |   $q-r$    | $\\operatorname{SSR}=\\operatorname{SSE}_r-\\operatorname{SSE}$ |  $\\operatorname{MSR}=\\operatorname{SSR}/(q-r)$  | $F=\\frac{\\operatorname{MSR}}{\\operatorname{MSE}}$ |\n|     Error     | $n-(q+1)$  |                     $\\operatorname{SSE}$                     | $\\operatorname{MSE}=\\operatorname{SSE}/(n-q-1)$ |                                                   |\n\n: 回归分析的方差分析 {#tbl-ranova}\n\n当$\\beta_1=\\cdots=\\beta_q=0$时，$x_t=\\beta_0+w_t$，有：\n\n$$\nR^2=\\frac{\\operatorname{SSE}_0-\\operatorname{SSE}}{\\operatorname{SSE}_0}\\\\\n\\operatorname{SSE}_0=\\sum_{t=1}^n(x_t-\\bar x)^2\n$$\n\n$R^2$是确定系数(coefficient of determination)。\n\n通过上述$F$检验来选择变量的纳入或删除，这就是逐步多元回归(stepwise multiple regression)。\n\n下面介绍通过模型优度来进行模型选择的方法。\n\n### 信息准则\n\n$k$个系数的正态回归模型，方差的最大似然估计可表示为：\n\n$$\n\\hat\\sigma_k^2=\\frac{\\operatorname{SSE}(k)}{n}\n$$\n\n其中$\\operatorname{SSE}(k)$表示$k$个回归系数的模型的残差平方和。显然$\\operatorname{SSE}(k)$会随着$k$的增加而单调减少。\n\n信息准则的基本思路是在最小化$\\hat\\sigma_k^2$的同时，添加一个随$k$的增加而单调增加的惩罚项，来选择合适的参数数量。\n\n#### Akaike信息准则(Akaike’s Information Criterion，AIC)\n\n$$\n\\operatorname{AIC}=\\log\\hat\\sigma_k^2+\\frac{n+2k}{n}\n$$\n\n最常用。\n\n#### AIC，偏差修正(AICc)\n\n$$\n\\operatorname{AICc}=\\log\\hat\\sigma_k^2+\\frac{n+k}{n-k-2}\n$$\n\n适用于参数数量相对较大，但样本量较小的情况。\n\n#### 贝叶斯信息准则(Bayesian Information Criterion，BIC)\n\n$$\n\\operatorname{BIC}=\\log\\hat\\sigma_k^2+\\frac{k\\log n}{n}\n$$\n\nBIC惩罚项的值远大于AIC，适合在大样本中选出较小的模型，防止过拟合。\n\n### 模型选择\n\n\n::: {.cell}\n::: {.cell-output-display}\n![洛杉矶地区平均每周心血管死亡率(上)、温度(中)、颗粒污染(下)](Chapter_2_files/figure-html/fig-card-1.png){#fig-card width=672}\n:::\n:::",
    "supporting": [
      "Chapter_2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}